{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78296a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Streamlit UI: Quickstart for Google Colab\n",
    "# Kill any previous Streamlit or cloudflared processes (safe in Colab)\n",
    "!pkill -f streamlit || true\n",
    "!pkill -f cloudflared || true\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "\n",
    "# Wait a bit to ensure ports are released\n",
    "time.sleep(2)\n",
    "\n",
    "# Find a free port (prefer 8080, else pick 8081-8099)\n",
    "def get_free_port(preferred=8080, fallback_range=(8081, 8099)):\n",
    "    s = socket.socket()\n",
    "    try:\n",
    "        s.bind((\"\", preferred))\n",
    "        port = preferred\n",
    "    except OSError:\n",
    "        for p in range(*fallback_range):\n",
    "            try:\n",
    "                s.bind((\"\", p))\n",
    "                port = p\n",
    "                break\n",
    "            except OSError:\n",
    "                continue\n",
    "        else:\n",
    "            raise RuntimeError(\"No free port found in range 8080-8099\")\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "PORT = get_free_port()\n",
    "print(f\"[INFO] Using port {PORT} for Streamlit and Cloudflared.\")\n",
    "\n",
    "# 1. Clone your repo (published repo URL)\n",
    "!git clone https://github.com/melbinjp/Lora_trainer.git\n",
    "%cd Lora_trainer\n",
    "\n",
    "# 2. Run the smart setup script\n",
    "!bash setup.sh\n",
    "\n",
    "# 3. Install cloudflared for public tunneling\n",
    "!wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
    "!chmod +x cloudflared\n",
    "\n",
    "# 4. Start Streamlit in the background and wait for it to be ready\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "streamlit_proc = subprocess.Popen(\n",
    "    ['streamlit', 'run', 'test.py', f'--server.port={PORT}', '--server.enableCORS=false'],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "# Wait for Streamlit to start (look for 'You can now view your Streamlit app')\n",
    "ready = False\n",
    "for i in range(60):\n",
    "    line = streamlit_proc.stdout.readline().decode()\n",
    "    print(line, end='')\n",
    "    if 'You can now view your Streamlit app' in line:\n",
    "        ready = True\n",
    "        break\n",
    "    time.sleep(1)\n",
    "if not ready:\n",
    "    print('Streamlit did not start in time. Check the logs above.')\n",
    "\n",
    "# 5. Start cloudflared tunnel and print the public URL\n",
    "if ready:\n",
    "    tunnel = subprocess.Popen(['./cloudflared', 'tunnel', '--url', f'http://localhost:{PORT}'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    public_url = None\n",
    "    for i in range(60):\n",
    "        line = tunnel.stdout.readline().decode()\n",
    "        print(line, end='')\n",
    "        if 'trycloudflare.com' in line:\n",
    "            public_url = re.search(r'https://[\\w\\-]+\\.trycloudflare.com', line)\n",
    "            if public_url:\n",
    "                print(f'\\nYour public Streamlit URL: {public_url.group(0)}')\n",
    "                break\n",
    "        time.sleep(1)\n",
    "    if not public_url:\n",
    "        print('Cloudflared tunnel failed to start. Please check the output above or try restarting the Colab runtime and running all cells again.')\n",
    "else:\n",
    "    print('Cloudflared tunnel not started because Streamlit did not start.')\n",
    "\n",
    "# Note: The Streamlit UI will be available at the printed public URL above if successful.\n",
    "# If the URL does not work, try restarting the Colab runtime and running all cells again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware & PyTorch Version Check (Recommended)\n",
    "import torch\n",
    "import platform\n",
    "import os\n",
    "\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        import subprocess\n",
    "        output = subprocess.check_output(['nvcc', '--version']).decode()\n",
    "        for line in output.split('\\n'):\n",
    "            if 'release' in line:\n",
    "                return line.strip().split('release')[-1].split(',')[0].strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "torch_version = torch.__version__\n",
    "cuda_version = get_cuda_version() if cuda_available else None\n",
    "\n",
    "print(f\"[INFO] Detected torch version: {torch_version}\")\n",
    "if cuda_available:\n",
    "    print(\"[INFO] CUDA GPU detected.\")\n",
    "    print(f\"[INFO] CUDA version: {cuda_version if cuda_version else 'Unknown'}\")\n",
    "    # Recommend best torch version for Colab CUDA 11.8\n",
    "    print(\"[RECOMMEND] For best performance on Colab, use torch 2.0.0+cu118:\")\n",
    "    print(\"  pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 --index-url https://download.pytorch.org/whl/cu118\")\n",
    "elif mps_available:\n",
    "    print(\"[INFO] Apple Silicon (MPS) detected.\")\n",
    "    print(\"[RECOMMEND] For best performance, use torch>=1.12.0:\")\n",
    "    print(\"  pip install torch>=1.12.0\")\n",
    "else:\n",
    "    print(\"[INFO] No GPU/NPU detected. Using CPU.\")\n",
    "    print(\"[RECOMMEND] For best performance, use the latest torch version:\")\n",
    "    print(\"  pip install torch --upgrade\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note on UI Modes\n",
    "\n",
    "The application now starts in **Simple Mode** by default. This mode is designed to be as user-friendly as possible, with most settings automated for you.\n",
    "\n",
    "If you are an advanced user and need access to detailed settings like learning rate, schedulers, or custom models, you can easily switch to **Advanced Mode** using the toggle button at the top of the application's UI."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
